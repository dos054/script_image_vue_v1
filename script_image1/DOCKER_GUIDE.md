# Ollama Docker ì‚¬ìš© ê°€ì´ë“œ

## ğŸ³ Dockerë¡œ Ollama ì‹¤í–‰í•˜ê¸°

### 1. Docker Composeë¡œ ì‹¤í–‰ (ê¶Œì¥)

```bash
# í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì‹¤í–‰
docker-compose up -d

# ë¡œê·¸ í™•ì¸
docker-compose logs -f ollama

# ì¤‘ì§€
docker-compose down

# ì™„ì „ ì‚­ì œ (ë³¼ë¥¨ í¬í•¨)
docker-compose down -v
```

### 2. ì§ì ‘ Docker ëª…ë ¹ì–´ë¡œ ì‹¤í–‰

```bash
# Ollama ì»¨í…Œì´ë„ˆ ì‹¤í–‰
docker run -d \
  --name ollama \
  -p 11434:11434 \
  -v ollama_data:/root/.ollama \
  ollama/ollama:latest

# ì»¨í…Œì´ë„ˆ ìƒíƒœ í™•ì¸
docker ps

# ë¡œê·¸ í™•ì¸
docker logs -f ollama

# ì¤‘ì§€
docker stop ollama

# ì¬ì‹œì‘
docker start ollama

# ì‚­ì œ
docker rm -f ollama
```

### 3. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

#### ë°©ë²• 1: docker exec ì‚¬ìš© (ê¶Œì¥)
```bash
# llama2 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
docker exec -it ollama ollama pull llama2

# mistral ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
docker exec -it ollama ollama pull mistral

# ë‹¤ë¥¸ ëª¨ë¸ë“¤
docker exec -it ollama ollama pull codellama
docker exec -it ollama ollama pull orca-mini
docker exec -it ollama ollama pull phi

# ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ ëª©ë¡ í™•ì¸
docker exec -it ollama ollama list
```

#### ë°©ë²• 2: ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ì ‘ì†
```bash
# ì»¨í…Œì´ë„ˆ ë‚´ë¶€ë¡œ ì§„ì…
docker exec -it ollama bash

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
ollama pull llama2
ollama list
exit
```

### 4. ëª¨ë¸ í…ŒìŠ¤íŠ¸

```bash
# ëª¨ë¸ ì‹¤í–‰ í…ŒìŠ¤íŠ¸
docker exec -it ollama ollama run llama2 "Hello, how are you?"

# í•œê¸€ í…ŒìŠ¤íŠ¸
docker exec -it ollama ollama run llama2 "ì•ˆë…•í•˜ì„¸ìš”"
```

### 5. API ì—°ê²° í™•ì¸

```bash
# curlë¡œ API í…ŒìŠ¤íŠ¸
curl http://localhost:11434/api/tags

# ìƒì„± í…ŒìŠ¤íŠ¸
curl -X POST http://localhost:11434/api/generate -d '{
  "model": "llama2",
  "prompt": "Why is the sky blue?"
}'
```

---

## ğŸ”§ IntelliJì—ì„œ ì‚¬ìš©í•˜ê¸°

### 1. Docker Compose ì‹¤í–‰ (IntelliJ ë‚´ë¶€)

1. IntelliJì—ì„œ `docker-compose.yml` íŒŒì¼ ì—´ê¸°
2. íŒŒì¼ ì¢Œì¸¡ì˜ ë…¹ìƒ‰ ì‹¤í–‰ ë²„íŠ¼ í´ë¦­
3. "Run 'docker-compose.yml'" ì„ íƒ

ë˜ëŠ” í„°ë¯¸ë„ì—ì„œ:
```bash
docker-compose up -d
```

### 2. Services íƒ­ì—ì„œ ê´€ë¦¬

1. IntelliJ í•˜ë‹¨ì˜ **Services** íƒ­ í´ë¦­
2. Docker ì„¹ì…˜ì—ì„œ `ollama` ì»¨í…Œì´ë„ˆ í™•ì¸
3. ìš°í´ë¦­ìœ¼ë¡œ ì‹œì‘/ì¤‘ì§€/ë¡œê·¸ í™•ì¸ ê°€ëŠ¥

### 3. Spring Boot ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰

1. ë¨¼ì € Ollama ì»¨í…Œì´ë„ˆê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
2. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ í™•ì¸
   ```bash
   docker exec -it ollama ollama list
   ```
3. IntelliJì—ì„œ `Script1Application` ì‹¤í–‰

---

## ğŸ“ application.yml ì„¤ì • (ì´ë¯¸ ì ìš©ë¨)

```yaml
ollama:
  host: http://localhost:11434  # Docker ì»¨í…Œì´ë„ˆ í¬íŠ¸
  model: llama2                  # ë‹¤ìš´ë¡œë“œí•œ ëª¨ë¸ëª…
  timeout: 300
```

---

## ğŸ› íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 1. í¬íŠ¸ ì¶©ëŒ (11434 í¬íŠ¸ê°€ ì´ë¯¸ ì‚¬ìš© ì¤‘)
```bash
# ì‚¬ìš© ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ í™•ì¸
netstat -ano | findstr :11434

# ë‹¤ë¥¸ í¬íŠ¸ë¡œ ë³€ê²½ (docker-compose.yml)
ports:
  - "11435:11434"  # í˜¸ìŠ¤íŠ¸:ì»¨í…Œì´ë„ˆ

# application.ymlë„ ë³€ê²½
ollama:
  host: http://localhost:11435
```

### 2. ì»¨í…Œì´ë„ˆê°€ ì‹œì‘ë˜ì§€ ì•ŠìŒ
```bash
# ë¡œê·¸ í™•ì¸
docker logs ollama

# ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘
docker restart ollama
```

### 3. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ëŠë¦¼
- ëª¨ë¸ í¬ê¸°ê°€ í¼ (llama2: ~3.8GB)
- ë„¤íŠ¸ì›Œí¬ ì†ë„ì— ë”°ë¼ ì‹œê°„ ì†Œìš”
- ë” ì‘ì€ ëª¨ë¸ ì‹œë„: `orca-mini` (~1.8GB)

```bash
docker exec -it ollama ollama pull orca-mini
```

### 4. ë©”ëª¨ë¦¬ ë¶€ì¡±
- OllamaëŠ” ìµœì†Œ 8GB RAM ê¶Œì¥
- Docker Desktopì˜ ë©”ëª¨ë¦¬ í• ë‹¹ ì¦ê°€:
  1. Docker Desktop ì„¤ì •
  2. Resources â†’ Memory ì¦ê°€ (8GB ì´ìƒ)

### 5. ì—°ê²° ì‹¤íŒ¨ (Connection refused)
```bash
# Ollama ìƒíƒœ í™•ì¸
docker ps

# API ì‘ë‹µ í™•ì¸
curl http://localhost:11434/api/tags

# ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘
docker restart ollama
```

---

## ğŸ’¡ ì¶”ì²œ ëª¨ë¸

### ì‘ê³  ë¹ ë¥¸ ëª¨ë¸ (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)
- **orca-mini** (~1.8GB) - ë¹ ë¥¸ ì‘ë‹µ
- **phi** (~1.6GB) - Microsoft ëª¨ë¸

### ê· í˜•ì¡íŒ ëª¨ë¸
- **llama2** (~3.8GB) - ê¸°ë³¸ ì¶”ì²œ
- **mistral** (~4.1GB) - ì¢‹ì€ ì„±ëŠ¥

### ì½”ë”© íŠ¹í™”
- **codellama** (~3.8GB) - ì½”ë“œ ìƒì„±

### í•œêµ­ì–´ ì§€ì›
- **llama2** - ê¸°ë³¸ í•œêµ­ì–´ ì§€ì›
- **EEVE-Korean** ë“± í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥

---

## ğŸ“Œ ë¹ ë¥¸ ì‹œì‘ ì²´í¬ë¦¬ìŠ¤íŠ¸

```bash
# 1. Docker Compose ì‹¤í–‰
docker-compose up -d

# 2. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
docker exec -it ollama ollama pull llama2

# 3. ë‹¤ìš´ë¡œë“œ í™•ì¸
docker exec -it ollama ollama list

# 4. API í…ŒìŠ¤íŠ¸
curl http://localhost:11434/api/tags

# 5. Spring Boot ì‹¤í–‰ (IntelliJ)
# Script1Application.java ì‹¤í–‰

# 6. ë¸Œë¼ìš°ì € ì ‘ì†
# http://localhost:8080
```

---

## ğŸ”„ ë°ì´í„° ê´€ë¦¬

### ëª¨ë¸ íŒŒì¼ ìœ„ì¹˜ (ë³¼ë¥¨)
```bash
# ë³¼ë¥¨ í™•ì¸
docker volume ls

# ë³¼ë¥¨ ìƒì„¸ ì •ë³´
docker volume inspect ollama_data

# ë³¼ë¥¨ ë°±ì—…
docker run --rm -v ollama_data:/data -v $(pwd):/backup \
  busybox tar czf /backup/ollama_backup.tar.gz /data
```

### ëª¨ë¸ ì‚­ì œ
```bash
# íŠ¹ì • ëª¨ë¸ ì‚­ì œ
docker exec -it ollama ollama rm llama2

# ì „ì²´ ì‚­ì œ (ë³¼ë¥¨ ì‚­ì œ)
docker-compose down -v
```

---

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

1. **ëª¨ë¸ ë³€ê²½ ì‹œ**:
   - `application.yml`ì˜ `ollama.model` ê°’ ë³€ê²½
   - Spring Boot ì¬ì‹œì‘

2. **ì„±ëŠ¥ ê°œì„ **:
   - GPU ì‚¬ìš© (NVIDIA GPU í•„ìš”)
   - `docker-compose.yml`ì˜ GPU ì„¹ì…˜ ì£¼ì„ í•´ì œ

3. **í”„ë¡œë•ì…˜ ë°°í¬**:
   - ë³„ë„ ì„œë²„ì— Ollama ë°°í¬
   - `application.yml`ì˜ hostë¥¼ ì™¸ë¶€ ì£¼ì†Œë¡œ ë³€ê²½

---

**ì´ì œ IntelliJì—ì„œ Dockerë¡œ Ollamaë¥¼ ì‹¤í–‰í•˜ê³  ê°œë°œí•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤!** ğŸš€
